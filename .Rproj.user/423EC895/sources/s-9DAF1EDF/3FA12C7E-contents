---
title: "Intro to DS - Linear Model part I"
author: "Ricardo Diaz"
date: "r Sys.Date()"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# HW assignment

## Linear model - quantitative regressors 

### Question 1  
**Import the data, call it `bikeorig.`**  
The `Date` variable is probably imported as factor level variable. In any case, let us remove `Date`, `Casual.Users`, 
and `Registered.Users` in the dataset and save it as a new datafame, call it `bike`. How many variables are in `bike`? 
How many of them are imported as `int`? Feel free to rename longer variable names into shorter ones for convenience.  
```{r}
data <- data.frame(read.csv('bikedata.csv' ))
str(data)
drop <- c('Date','Casual.Users', 'Registered.Users')
bike = data[!(names(data)%in% drop)]
str(bike)

sapply(bike, is.integer)
```
### Answer Question 1
There are 11 varibles after we drop the specified columns.
9 variables are imported as int

### Question 2    
**Select only the subset with `Hour` equal 16 only. Call it `bike16`**  
These are the afternoon rush hour data. How many observations are there?   

```{r}
bike16 <- subset(bike, Hour == 16)
#bike16
```
### Answer Question 2
There are 730 observations for rush hour data.

### Question 3  
**Before building any models, we should make sure the variables are set up properly.**  
(This problem is solved for you. Codes are given below.)  
Which ones should be recorded as categorical? Convert them now before we proceed to the model building.  

Note: After converting, the correlation function `cor()` will not work with categorical/factor variables. 
I would keep the original `bike16` dataframe as numeric, and use that to 
find the correlation matrix. 
Although technically correlation between categorical and numeric variables are not 
well defined in general, we can still get some useful information if the 
categorical variable is at least at ordinal level. See future discussion 
on using "Pearson" vs "Spearman" methods for correlation tests. 

While the `cor()` function does not accept categorical variables (and therefore 
we cannot use it for `corrplot()`), the `lattice::pairs()` function does not complain 
about categorical columns. We can still use it to get a visual distribution of 
data values from it.
 

```{r}
bike_final = bike16
bike_final$Season = factor(bike16$Season)
#bike_final$Hour = factor(bike16$Hour)
bike_final$Holiday = factor(bike16$Holiday)
bike_final$Day = factor(bike16$Day)
bike_final$Working.Day = factor(bike16$Working.Day)
bike_final$Weather = factor(bike16$Weather)
str(bike_final)
```

We decided to convert these variables into categorical (factor):  
`Season`, `Holiday`, `Day`, `Workday`, and `Weather`.  Notice that 
the dataframe `bike16` still has all variables numerical, while the df `bike_final` 
include categorical columns that we just converted. 

### Question 4  
**Make a `pairs()` plot with all the variables (quantitative and qualitative).**  

```{r}
loadPkg("lattice") 

pairs(bike_final[1:13])

### bike16 is numeric / can use corplot corr
## bikefinal is factors / can use pairs()
```



### Question 5  
**Make a `corrplot()` with only the numerical variables.**  
You can either subset the df with only numerical variables first, then create 
the create the cor-matrix to plot. Or you can create the cor-matrix from 
`bike16`, then select select out the portion of the matrix that you want. 
Use options that shows well the relationships between different variables. 
 

```{r}
loadPkg("corrplot")
#str(bike16)
drop2 <- c('Hour')
df2 = bike16[,!(names(bike16)%in% drop2)]
bike16cor = cor(df2)
xkabledply(bike16cor)

corrplot(bike16cor, method = 'ellipse', type = 'upper', title = 'Correlation  Map')

#corrplot(bike16cor, method = 'number')
#mtcarscor = cor(mtcars) # get the correlation matrix between all numerical variables.
#mtcarscor

```



### Question 6   
**By using numerical variables only, build a linear model with 1 independent variable to predict the `Total Users`.**  
Choose the variable with the strongest correlation coefficient. Make some short 
comments on the coefficient values, their p-values, and the multiple R-squared value.  

```{r}

linearmodel1 <- lm(Total.Users ~ Temperature.F, data = bike16)
summary(linearmodel1)
xkabledply(linearmodel1,)
coef(linearmodel1)


```
### Answer Question 6

Our p value is very small which explains there is a correlation between Total Users and Temperature. We can say there is a significant relationship between the variables. Our coefficient coefficient is -0.154 which mean we have negative correlation but our relationship is weak.
Our R square is how close each data points first to the regression line, in this case we have a 0.303 R squared , which is low but acceptable . 
33% of the variation in Total Users is accounted for its regression on Temperature. 

### Question 7   
**Next, add a second variable to the model.**  
Choose the variable with the next strongest correlation, but avoid using obviously 
collinear variables. When you have the model, 
check the VIF values. If the VIF is higher than 5, discard this model, and try the 
variable with the next strongest correlation until you find one that works 
(ideally with vif’s <5, or if you have to, allow vif up to 10). Again, 
comment on the coefficient values, their p-values, 
and the multiple R-squared value.  

```{r}


str(bike16)


linearmodel2 <- lm(Total.Users ~ Temperature.Feels.F + Humidity , data =bike16)
#linearmodel2 <- lm(Total.Users ~ Humidity + Temperature.Feels.F, data =bike16)
#linearmodel2 <- lm(Total.Users ~ Day.of.the.Week + Temperature.Feels.F, data =bike16)
xkablevif(linearmodel2, title = 'VIF values')
summary(linearmodel2)
xkabledply(linearmodel2,)
coef(linearmodel2)



```
### Answer Question 7

We decided our best candidates are Temperature F and Humidity, which both have VIF values of 1.01 which represents midly correlation which is acceptable. Our pvalues are low which represents significant relationships between variables. Our R squared we have a 0.341 R squared , which is low but acceptable . 
34 % of the variation in Total Users is accounted for its regression on Temperature F and Humidity. 


### Question 8  
**We will try one more time as in the previous question, to add a third variable in our model.**  

```{r}

#linearmodel3 <- lm(Total.Users ~ Humidity + Temperature.Feels.F + Season, data =bike16)
linearmodel3 <- lm(Total.Users ~ Humidity + Temperature.Feels.F + Wind.Speed, data =bike16)

summary(linearmodel3)
xkablevif(linearmodel3, title = 'VIF values')
xkabledply(linearmodel3,)
str(bike16)

``` 

### Answer Question 8
We added Season as are our third variable , Our three candidates (Temperature Feels F, WindSpeed and Humidity) have acceptable VIF values which represents midly correlation. We found something interesting examining our pvalues. Temperature Feels F and Humidity are low which represents significant relationships for Total Users but in the case of Wind.Speed our pvalue is 0.03, its still below the significant level. Our R squared is 0.355 , which is low but acceptable. 35 % of the variation in Total Users is accounted for its regression on Temperature F , Humidity and WindSpeed. 

### Question 9  
**For the 3-variable model you found, find the confidence intervals of the coefficients.**  

```{r}

#cor.test(bike16$Total.Users,bike16$Humidity)
#cor.test(bike16$Total.Users,bike16$Temperature.Feels.F)
#cor.test(bike16$Total.Users,bike16$Wind.Speed) ## NOT SURE 

cofi <- confint(linearmodel3, level=0.95)
xkabledply(cofi)

```


### Question 10    
**Use ANOVA to compare the three different models you found.**  
You have found three different models. Use ANOVA test to compare their residuals. What conclusion can you draw?


```{r}


anova(linearmodel1, linearmodel2,linearmodel3)
#xkabledply(trial,title = "ANOVA comparison between the models")

#linearmodel1$residuals
#plant_aov = aov(weight ~ group, data = PlantGrowth)
#str(bike16)

```

### Answer Question 10
Our RSS we can conclude the amount of error between the regression function between models is not that different but our RSS is too big which mean our models may not be the best. We can conclude our second linear model is better to explain our Total Users. 